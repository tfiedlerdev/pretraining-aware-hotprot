{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-/Validation split creation\n",
    "In this notebook we go through our process of creating the train dataset as well as the validation dataset. \n",
    "In the context of this project a dataset means a set of `(protein sequence, thermostability)` pairs. \n",
    "For a given protein sequence there may be multiple thermostability value measurements as given by the FLIP dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Meltome dataset\n",
    "The source of the dataset file `full_dataset_sequences.fasta` is [here](https://github.com/J-SNACKKB/FLIP/tree/main/splits/meltome). Among other information it contains protein sequences and corresponding thermostability (melting point) measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta(filepath=\"data/full_dataset_sequences.fasta\"):\n",
    "    first = True\n",
    "    max = 0\n",
    "    dataset = []\n",
    "    with open(filepath) as fasta:\n",
    "        for line in fasta:\n",
    "            if line[0] == \">\":\n",
    "                if first:\n",
    "                    first = False\n",
    "                else:\n",
    "                    dataset.append(entry)\n",
    "                entry = {}\n",
    "                header_tokens = line.split(\" \")\n",
    "                entry[\"id\"] = header_tokens[0].replace(\">\", \"\").split(\"_\")[0]\n",
    "                entry[\"header\"] = line.replace(\"\\n\", \"\")\n",
    "                entry[\"temp\"] = float(header_tokens[1].split(\"=\")[1].replace(\"\\n\", \"\"))\n",
    "                entry[\"sequence\"] = \"\"\n",
    "            else:\n",
    "                entry[\"sequence\"] = entry[\"sequence\"] + line.replace(\"\\n\", \"\")\n",
    "                max = len(entry[\"sequence\"]) if len(entry[\"sequence\"]) > max else max\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "flip_dataset = read_fasta()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read ESM validation protein clusters\n",
    "ESM validation protein clusters are those Uniref100 clusters that were held out during training of ESM2 and ESMFold. \n",
    "As we are basically doing transfer learning on top of the ESMFold outputs, we also use these as a validation/test set, avoiding any potential data leakage due to ESMFold having seen proteins during training that we are using during validation/testing. \n",
    "More info and a download link can be found [here](https://github.com/facebookresearch/esm#pre-training-dataset-split--) .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_eval_clusters = dict()\n",
    "esm_eval_ids = set()\n",
    "with open(\"./data/uniref201803_ur100_valid_headers.txt\") as txt_file:\n",
    "    for line in txt_file:\n",
    "        parts = line.split(\" \")\n",
    "        id = parts[0].split(\"_\")[1]\n",
    "        cluster = parts[1].split(\"_\")[1].replace(\"\\n\", \"\")\n",
    "        esm_eval_ids.add(id)\n",
    "        if cluster not in esm_eval_clusters:\n",
    "            esm_eval_clusters[cluster] = []\n",
    "        esm_eval_clusters[cluster].append(id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create held out dataset (test/val) and train ids set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "held_out_dataset = []\n",
    "train_dataset = []\n",
    "train_ids = set()\n",
    "all_ids = set()\n",
    "dataset = read_fasta()\n",
    "for entry in dataset:\n",
    "    seq = entry[\"sequence\"]\n",
    "    id = entry[\"id\"]\n",
    "    all_ids.add(id)\n",
    "    if id in esm_eval_ids:\n",
    "        held_out_dataset.append(entry)\n",
    "    else:\n",
    "        train_dataset.append(entry)\n",
    "        train_ids.add(id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split held out dataset by cluster\n",
    "To avoid optimizing the hyperparameters of our model for the given validation set, we also introduce a test set. The test set is then only used for final evaluation of the model. However hyperparameters won't be optimized for best performance on the test set but on the validation set. \n",
    "\n",
    "To avoid any similar protein sequences being present in our test and validation set, we construct these sets by randomly splitting the held out set (i.e. the non-training set) by their UniRef100 cluster. While doing this each cluster has a 2/3rd chance to be added to the validation set and a 1/3rd chance of being added to the test set respectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = set()\n",
    "val_ids = set()\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "for cluster_id, protein_ids in esm_eval_clusters.items():\n",
    "    is_test = random.random() <= 1 / 3\n",
    "    for protein_id in protein_ids:\n",
    "        (test_ids if is_test else val_ids).add(protein_id)\n",
    "\n",
    "test_dataset = [item for item in held_out_dataset if item[\"id\"] in test_ids]\n",
    "val_dataset = [item for item in held_out_dataset if item[\"id\"] in val_ids]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store val.csv and train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeMetadata(ds, name: str):\n",
    "    with open(f\"./data/{name}.csv\", \"w\") as f:\n",
    "        f.write(\"sequence, melting point\\n\")\n",
    "        for entry in ds:\n",
    "            f.write(f'{entry[\"sequence\"]}, {entry[\"temp\"]}\\n')\n",
    "\n",
    "\n",
    "def print_ds_infos(name: str, ds: list):\n",
    "    unique_seqs = set([entry[\"sequence\"] for entry in ds])\n",
    "    print(f'{\"-\"*5}Info for {name} set{\"-\"*5}')\n",
    "    print(\"Num sequences:\", len(ds))\n",
    "    print(\"Num sequences (len < 3000):\", len([entry for entry in ds if len(entry[\"sequence\"]) < 3000]))\n",
    "    print(\"Num unique sequences: \", len(unique_seqs))\n",
    "    print(\n",
    "        \"Num unique sequences (len < 3000): \",\n",
    "        len([seq for seq in unique_seqs if len(seq) < 3000]),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print_ds_infos(\"train\", train_dataset)\n",
    "# print_ds_infos(\"val\", val_dataset)\n",
    "# print_ds_infos(\"test\", test_dataset)\n",
    "\n",
    "storeMetadata(train_dataset, \"train\")\n",
    "storeMetadata(val_dataset, \"val\")\n",
    "storeMetadata(test_dataset, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create our dataset with median measurements\n",
    "As our dataset contains mutliple melting point measurements per protein, there is no single true value for a given protein. Another approach than showing the model all measurements for a given protein would be to take the median, so that we don't optimize back and forth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Info for train_median set-----\n",
      "Num sequences: 30988\n",
      "Num sequences (len < 3000): 30844\n",
      "Num unique sequences:  30988\n",
      "Num unique sequences (len < 3000):  30844\n",
      "-----Info for val_median set-----\n",
      "Num sequences: 2129\n",
      "Num sequences (len < 3000): 2113\n",
      "Num unique sequences:  2129\n",
      "Num unique sequences (len < 3000):  2113\n",
      "-----Info for test_median set-----\n",
      "Num sequences: 1091\n",
      "Num sequences (len < 3000): 1082\n",
      "Num unique sequences:  1091\n",
      "Num unique sequences (len < 3000):  1082\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def poolMeasurements(ds):\n",
    "    \"\"\"\n",
    "    Pool measurements of a protein via median\n",
    "    \"\"\"\n",
    "    measurementsPerProtein = dict([(sample[\"sequence\"], []) for sample in ds])\n",
    "    for sample in ds:\n",
    "        measurementsPerProtein[sample[\"sequence\"]].append(sample[\"temp\"])\n",
    "    pooledDs = [{\"temp\": np.median(measurements), \"sequence\": sequence} for sequence, measurements in measurementsPerProtein.items()]\n",
    "    return pooledDs\n",
    "    \n",
    "train_median_ds = poolMeasurements(train_dataset)\n",
    "val_median_ds = poolMeasurements(val_dataset)\n",
    "test_median_ds = poolMeasurements(test_dataset)\n",
    "\n",
    "storeMetadata(train_median_ds, \"train_EPA\")\n",
    "storeMetadata(val_median_ds, \"val_EPA\")\n",
    "storeMetadata(test_median_ds, \"test_EPA\")\n",
    "print_ds_infos(\"train_EPA\", train_median_ds)\n",
    "print_ds_infos(\"val_EPA\", val_median_ds)\n",
    "print_ds_infos(\"test_EPA\", test_median_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert FLIP dataset split to our format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Info for train_FLIP set-----\n",
      "Num sequences: 22335\n",
      "Num sequences (len < 3000): 22225\n",
      "Num unique sequences:  18435\n",
      "Num unique sequences (len < 3000):  18349\n",
      "-----Info for val_FLIP set-----\n",
      "Num sequences: 2482\n",
      "Num sequences (len < 3000): 2466\n",
      "Num unique sequences:  2376\n",
      "Num unique sequences (len < 3000):  2362\n",
      "-----Info for test_FLIP set-----\n",
      "Num sequences: 3134\n",
      "Num sequences (len < 3000): 3115\n",
      "Num unique sequences:  3134\n",
      "Num unique sequences (len < 3000):  3115\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/mixed_split.csv\", \"r\") as f:\n",
    "    # columns sequence,target,set,validation\n",
    "    train = []\n",
    "    val = []\n",
    "    test = []\n",
    "\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        sequence, target, split, validation = line.strip().split(\",\")\n",
    "       \n",
    "       \n",
    "        if split == \"train\":\n",
    "            if validation==\"True\":\n",
    "                val.append({\"sequence\": sequence, \"temp\": target})\n",
    "            else: \n",
    "                train.append({\"sequence\": sequence, \"temp\": target})\n",
    "        elif split == \"test\":\n",
    "            test.append({\"sequence\": sequence, \"temp\": target})\n",
    "        else: \n",
    "            raise Exception(\"Invalid set\")\n",
    "    \n",
    "    storeMetadata(train, \"train_FLIP\")\n",
    "    storeMetadata(val, \"val_FLIP\")\n",
    "    storeMetadata(test, \"test_FLIP\")\n",
    "    print_ds_infos(\"train_FLIP\", train)\n",
    "    print_ds_infos(\"val_FLIP\", val)\n",
    "    print_ds_infos(\"test_FLIP\", test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
